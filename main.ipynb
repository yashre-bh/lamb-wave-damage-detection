{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "# import numpy as np\n",
    "from tqdm import tqdm\n",
    "# import random\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 2\n",
    "TIMESTEP = 2000\n",
    "LR = 3e-4\n",
    "BATCH_SIZE = 5\n",
    "NUM_WORKERS = 2\n",
    "BETA1 = 0.5\n",
    "BETA2 = 0.999\n",
    "SAVE_MODEL = True\n",
    "LOAD_MODEL = False\n",
    "NET1_CHK = \"/net1.path.tar\"\n",
    "NET2_CHK = \"/net2.path.tar\"\n",
    "NETCAT_CHK = \"/netcat.path.tar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input = []\n",
    "        self.output = []\n",
    "        \n",
    "        for _ in range(10):\n",
    "            self.input.append(torch.randn(2, 1925))\n",
    "            self.output.append(torch.rand(1))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return {'feature': self.input[index], 'target': self.output[index]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolute(nn.Module):\n",
    "    def __init__(self, in_filters, out_filters, kernel_size, dropout=0.0, maxpool=0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_filters, out_filters, kernel_size),\n",
    "            nn.BatchNorm1d(out_filters),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        if dropout!=0: self.conv.append(nn.Dropout(dropout))\n",
    "        if maxpool!=0: self.conv.append(nn.MaxPool1d(maxpool))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class DNF(nn.Module):\n",
    "    def __init__(self, in_filters, out_filters, do_transpose=False, do_flatten=False):\n",
    "        super().__init__()\n",
    "        self.do_transpose = do_transpose\n",
    "        self.dnf = nn.Sequential(nn.Linear(in_filters, out_filters))\n",
    "        if do_flatten: self.dnf.append(nn.Flatten())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.do_transpose: x = torch.transpose(x, 2, 1)\n",
    "        return self.dnf(x)\n",
    "\n",
    "\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, in_filters, hidden_filters=50):\n",
    "        super().__init__()\n",
    "        self.ann = nn.Sequential(\n",
    "            nn.Linear(in_filters, hidden_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_filters, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ann(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Convolute(2, 576, 11)\n",
    "        self.conv2 = Convolute(576, 484, 11, 0.3, 4)\n",
    "        self.conv3 = Convolute(484, 400, 5)\n",
    "        self.conv4 = Convolute(400, 324, 5, 0.2)\n",
    "        self.conv5 = DNF(324, 256, True, True)\n",
    "        self.conv6 = DNF(119808, 150)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        return self.conv6(x)\n",
    "\n",
    "class NetCat(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ann1 = ANN(300)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ann1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optim, filename=\"/checkpoint.path.tar\"):\n",
    "    print(\"=> Saving Checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optim_state\": optim.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optim, lr):\n",
    "    print(\"=> Loading Checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    optim.load_state_dict(checkpoint[\"optim_state\"])\n",
    "    for group in optim.param_groups:\n",
    "        group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, net1, net2, netcat, net1_scaler, net2_scaler, netcat_scaler, net1_optim, net2_optim, netcat_optim, loss_function):\n",
    "    losses = []\n",
    "    data_loop = tqdm(data_loader, leave=True)\n",
    "\n",
    "    net1.zero_grad()\n",
    "    net2.zero_grad()\n",
    "    netcat.zero_grad()\n",
    "\n",
    "    for data in data_loop:\n",
    "        features, targets = data['feature'].to(DEVICE), data['target'].to(DEVICE)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            dense1 = net1(features)\n",
    "            dense2 = net2(features)\n",
    "            dense = torch.cat([dense1, dense2], dim = 1)\n",
    "            preds = netcat(dense)\n",
    "            loss = loss_function(preds, targets)\n",
    "\n",
    "        net1.zero_grad()\n",
    "        net1_scaler.scale(loss).backward()\n",
    "        net1_scaler.step(net1_optim)\n",
    "        net1_scaler.update()\n",
    "        net2.zero_grad()\n",
    "        net2_scaler.scale(loss/16).backward()\n",
    "        net2_scaler.step(net2_optim)\n",
    "        net2_scaler.update()\n",
    "        netcat.zero_grad()\n",
    "        netcat_scaler.scale(loss/16).backward()\n",
    "        netcat_scaler.step(netcat_optim)\n",
    "        netcat_scaler.update()\n",
    "\n",
    "        losses.append(loss.data)\n",
    "\n",
    "    loss_avg = torch.mean(torch.FloatTensor(losses))\n",
    "    print(f'Average Loss this epoch = {loss_avg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m             save_checkpoint(netcat, netcat_optim, filename\u001b[39m=\u001b[39mNETCAT_CHK)\n\u001b[1;32m     27\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 28\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m():\n\u001b[1;32m      2\u001b[0m     net1, net2, netcat \u001b[39m=\u001b[39m Net(), Net(), NetCat()\n\u001b[0;32m----> 3\u001b[0m     net1, net2, netcat \u001b[39m=\u001b[39m net1\u001b[39m.\u001b[39;49mto(DEVICE), net2\u001b[39m.\u001b[39mto(DEVICE), netcat\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m      4\u001b[0m     net1_scaler, net2_scaler, netcat_scaler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mGradScaler(), torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mGradScaler(), torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mGradScaler()\n\u001b[1;32m      5\u001b[0m     net1_optim \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(net1\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mLR, betas\u001b[39m=\u001b[39m(BETA1, BETA2))\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:989\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    987\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 989\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:664\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 664\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    665\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    666\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:987\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    985\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 987\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:229\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron:\n\u001b[1;32m    228\u001b[0m     os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mLAZY\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 229\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    230\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    233\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    net1, net2, netcat = Net(), Net(), NetCat()\n",
    "    net1, net2, netcat = net1.to(DEVICE), net2.to(DEVICE), netcat.to(DEVICE)\n",
    "    net1_scaler, net2_scaler, netcat_scaler = torch.cuda.amp.GradScaler(), torch.cuda.amp.GradScaler(), torch.cuda.amp.GradScaler()\n",
    "    net1_optim = torch.optim.Adam(net1.parameters(), lr=LR, betas=(BETA1, BETA2))\n",
    "    net2_optim = torch.optim.Adam(net2.parameters(), lr=LR, betas=(BETA1, BETA2))\n",
    "    netcat_optim = torch.optim.Adam(netcat.parameters(), lr=LR, betas=(BETA1, BETA2))\n",
    "    loss_function = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    \n",
    "    train_data = WaveDataset()\n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(NET1_CHK, net1, net1_optim, lr=LR)\n",
    "        load_checkpoint(NET2_CHK, net2, net2_optim, lr=LR)\n",
    "        load_checkpoint(NETCAT_CHK, netcat, netcat_optim, lr=LR)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f'Epoch count = {epoch+1}')\n",
    "        train(train_loader, net1, net2, netcat, net1_scaler, net2_scaler, netcat_scaler, net1_optim, net2_optim, netcat_optim, loss_function)\n",
    "        if SAVE_MODEL and (epoch+1) % 5 == 0:\n",
    "            save_checkpoint(net1, net1_optim, filename=NET1_CHK)\n",
    "            save_checkpoint(net2, net2_optim, filename=NET2_CHK)\n",
    "            save_checkpoint(netcat, netcat_optim, filename=NETCAT_CHK)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1=torch.randn(1, 2, 1925)\n",
    "# head1 = []\n",
    "# head1.append(Convolute(2, 576, 11))\n",
    "# head1.append(Convolute(576, 484, 11, 0.3, 4))\n",
    "# head1.append(Convolute(484, 400, 5))\n",
    "# head1.append(Convolute(400, 324, 5, 0.2))\n",
    "# head1.append(DNF(324, 256, True, True))\n",
    "# head1.append(DNF(119808, 150))\n",
    "# for obj in head1: x1=obj(x1)\n",
    "# x1.shape\n",
    "\n",
    "# x2=torch.randn(1, 2, 1925)\n",
    "# head2 = []\n",
    "# head2.append(Convolute(2, 576, 11))\n",
    "# head2.append(Convolute(576, 484, 11, 0.3, 4))\n",
    "# head2.append(Convolute(484, 400, 5))\n",
    "# head2.append(Convolute(400, 324, 5, 0.2))\n",
    "# head2.append(DNF(324, 256, True, True))\n",
    "# head2.append(DNF(119808, 150))\n",
    "# for obj in head2: x2=obj(x2)\n",
    "# x2.shape\n",
    "\n",
    "# x = torch.cat([x1, x2], dim=1)\n",
    "# x.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f4f1ecbe28321d1e7e48994d3c3c20c7e5b7c7d18dcc157e526614cd30d2d48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
