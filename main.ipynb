{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shrey\\miniconda3\\envs\\lbp\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "# import numpy as np\n",
    "from tqdm import tqdm\n",
    "# import random\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")\n",
    "EPOCHS = 2\n",
    "TIMESTEP = 2000\n",
    "LR = 3e-4\n",
    "BATCH_SIZE = 5\n",
    "NUM_WORKERS = 2\n",
    "BETA1 = 0.5\n",
    "BETA2 = 0.999\n",
    "SAVE_MODEL = True\n",
    "LOAD_MODEL = False\n",
    "NET1_CHK = \"/net1.path.tar\"\n",
    "NET2_CHK = \"/net2.path.tar\"\n",
    "NETCAT_CHK = \"/netcat.path.tar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input = []\n",
    "        self.output = []\n",
    "        \n",
    "        for _ in range(10):\n",
    "            self.input.append(torch.randn(2, 1925))\n",
    "            self.output.append(torch.rand(1))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return {'feature': self.input[index], 'target': self.output[index]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolute(nn.Module):\n",
    "    def __init__(self, in_filters, out_filters, kernel_size, dropout=0.0, maxpool=0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_filters, out_filters, kernel_size),\n",
    "            nn.BatchNorm1d(out_filters),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        if dropout!=0: self.conv.append(nn.Dropout(dropout))\n",
    "        if maxpool!=0: self.conv.append(nn.MaxPool1d(maxpool))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class DNF(nn.Module):\n",
    "    def __init__(self, in_filters, out_filters, do_transpose=False, do_flatten=False):\n",
    "        super().__init__()\n",
    "        self.do_transpose = do_transpose\n",
    "        self.dnf = nn.Sequential(nn.Linear(in_filters, out_filters))\n",
    "        if do_flatten: self.dnf.append(nn.Flatten())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.do_transpose: x = torch.transpose(x, 2, 1)\n",
    "        return self.dnf(x)\n",
    "\n",
    "\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, in_filters, hidden_filters=50):\n",
    "        super().__init__()\n",
    "        self.ann = nn.Sequential(\n",
    "            nn.Linear(in_filters, hidden_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_filters, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ann(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Convolute(2, 576, 11)\n",
    "        self.conv2 = Convolute(576, 484, 11, 0.3, 4)\n",
    "        self.conv3 = Convolute(484, 400, 5)\n",
    "        self.conv4 = Convolute(400, 324, 5, 0.2)\n",
    "        self.conv5 = DNF(324, 256, True, True)\n",
    "        self.conv6 = DNF(119808, 150)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        return self.conv6(x)\n",
    "\n",
    "class NetCat(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ann1 = ANN(300)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ann1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optim, filename=\"/checkpoint.path.tar\"):\n",
    "    print(\"=> Saving Checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optim_state\": optim.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optim, lr):\n",
    "    print(\"=> Loading Checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    optim.load_state_dict(checkpoint[\"optim_state\"])\n",
    "    for group in optim.param_groups:\n",
    "        group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, net1, net2, netcat, net1_scaler, net2_scaler, netcat_scaler, net1_optim, net2_optim, netcat_optim, loss_function):\n",
    "    losses = []\n",
    "    data_loop = tqdm(data_loader, leave=True)\n",
    "\n",
    "    net1.zero_grad()\n",
    "    net2.zero_grad()\n",
    "    netcat.zero_grad()\n",
    "\n",
    "    for data in data_loop:\n",
    "        features, targets = data['feature'].to(DEVICE), data['target'].to(DEVICE)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            dense1 = net1(features)\n",
    "            dense2 = net2(features)\n",
    "            dense = torch.cat([dense1, dense2], dim = 1)\n",
    "            preds = netcat(dense)\n",
    "            loss = loss_function(preds, targets)\n",
    "\n",
    "        net1.zero_grad()\n",
    "        net1_scaler.scale(loss).backward()\n",
    "        net1_scaler.step(net1_optim)\n",
    "        net1_scaler.update()\n",
    "        net2.zero_grad()\n",
    "        net2_scaler.scale(loss/16).backward()\n",
    "        net2_scaler.step(net2_optim)\n",
    "        net2_scaler.update()\n",
    "        netcat.zero_grad()\n",
    "        netcat_scaler.scale(loss/16).backward()\n",
    "        netcat_scaler.step(netcat_optim)\n",
    "        netcat_scaler.update()\n",
    "\n",
    "        losses.append(loss.data)\n",
    "\n",
    "    loss_avg = torch.mean(torch.FloatTensor(losses))\n",
    "    print(f'Average Loss this epoch = {loss_avg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch count = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m             save_checkpoint(netcat, netcat_optim, filename\u001b[39m=\u001b[39mNETCAT_CHK)\n\u001b[0;32m     27\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 28\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn [8], line 21\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m     20\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch count = \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m     train(train_loader, net1, net2, netcat, net1_scaler, net2_scaler, netcat_scaler, net1_optim, net2_optim, netcat_optim, loss_function)\n\u001b[0;32m     22\u001b[0m     \u001b[39mif\u001b[39;00m SAVE_MODEL \u001b[39mand\u001b[39;00m (epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     23\u001b[0m         save_checkpoint(net1, net1_optim, filename\u001b[39m=\u001b[39mNET1_CHK)\n",
      "Cell \u001b[1;32mIn [7], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(data_loader, net1, net2, netcat, net1_scaler, net2_scaler, netcat_scaler, net1_optim, net2_optim, netcat_optim, loss_function)\u001b[0m\n\u001b[0;32m     22\u001b[0m net1_scaler\u001b[39m.\u001b[39mupdate()\n\u001b[0;32m     23\u001b[0m net2\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 24\u001b[0m net2_scaler\u001b[39m.\u001b[39;49mscale(loss\u001b[39m/\u001b[39;49m\u001b[39m16\u001b[39;49m)\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     25\u001b[0m net2_scaler\u001b[39m.\u001b[39mstep(net2_optim)\n\u001b[0;32m     26\u001b[0m net2_scaler\u001b[39m.\u001b[39mupdate()\n",
      "File \u001b[1;32mc:\\Users\\shrey\\miniconda3\\envs\\lbp\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\shrey\\miniconda3\\envs\\lbp\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    net1, net2, netcat = Net(), Net(), NetCat()\n",
    "    net1, net2, netcat = net1.to(DEVICE), net2.to(DEVICE), netcat.to(DEVICE)\n",
    "    net1_scaler, net2_scaler, netcat_scaler = torch.cuda.amp.GradScaler(), torch.cuda.amp.GradScaler(), torch.cuda.amp.GradScaler()\n",
    "    net1_optim = torch.optim.Adam(net1.parameters(), lr=LR, betas=(BETA1, BETA2))\n",
    "    net2_optim = torch.optim.Adam(net2.parameters(), lr=LR, betas=(BETA1, BETA2))\n",
    "    netcat_optim = torch.optim.Adam(netcat.parameters(), lr=LR, betas=(BETA1, BETA2))\n",
    "    loss_function = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    \n",
    "    train_data = WaveDataset()\n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(NET1_CHK, net1, net1_optim, lr=LR)\n",
    "        load_checkpoint(NET2_CHK, net2, net2_optim, lr=LR)\n",
    "        load_checkpoint(NETCAT_CHK, netcat, netcat_optim, lr=LR)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f'Epoch count = {epoch+1}')\n",
    "        train(train_loader, net1, net2, netcat, net1_scaler, net2_scaler, netcat_scaler, net1_optim, net2_optim, netcat_optim, loss_function)\n",
    "        if SAVE_MODEL and (epoch+1) % 5 == 0:\n",
    "            save_checkpoint(net1, net1_optim, filename=NET1_CHK)\n",
    "            save_checkpoint(net2, net2_optim, filename=NET2_CHK)\n",
    "            save_checkpoint(netcat, netcat_optim, filename=NETCAT_CHK)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1=torch.randn(1, 2, 1925)\n",
    "# head1 = []\n",
    "# head1.append(Convolute(2, 576, 11))\n",
    "# head1.append(Convolute(576, 484, 11, 0.3, 4))\n",
    "# head1.append(Convolute(484, 400, 5))\n",
    "# head1.append(Convolute(400, 324, 5, 0.2))\n",
    "# head1.append(DNF(324, 256, True, True))\n",
    "# head1.append(DNF(119808, 150))\n",
    "# for obj in head1: x1=obj(x1)\n",
    "# x1.shape\n",
    "\n",
    "# x2=torch.randn(1, 2, 1925)\n",
    "# head2 = []\n",
    "# head2.append(Convolute(2, 576, 11))\n",
    "# head2.append(Convolute(576, 484, 11, 0.3, 4))\n",
    "# head2.append(Convolute(484, 400, 5))\n",
    "# head2.append(Convolute(400, 324, 5, 0.2))\n",
    "# head2.append(DNF(324, 256, True, True))\n",
    "# head2.append(DNF(119808, 150))\n",
    "# for obj in head2: x2=obj(x2)\n",
    "# x2.shape\n",
    "\n",
    "# x = torch.cat([x1, x2], dim=1)\n",
    "# x.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lbp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ca05d5b013b6acfe4e897f654a2a6ca558fd0f228b242f8f9d0c928cdeb5790"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
