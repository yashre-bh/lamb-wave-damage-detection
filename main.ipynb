{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 2\n",
    "TIMESTEP = 2000\n",
    "LR = 3e-4\n",
    "BATCH_SIZE = 5\n",
    "NUM_WORKERS = 2\n",
    "BETA1 = 0.5\n",
    "BETA2 = 0.999\n",
    "SAVE_MODEL = True\n",
    "LOAD_MODEL = False\n",
    "NET1_CHK = \"/net1.path.tar\"\n",
    "NET2_CHK = \"/net2.path.tar\"\n",
    "NETCAT_CHK = \"/netcat.path.tar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input = []\n",
    "        self.output = []\n",
    "\n",
    "        for _ in range(10):\n",
    "            self.input.append(torch.randn(2, 1925))\n",
    "            self.output.append(torch.rand(1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {'feature': self.input[index], 'target': self.output[index]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolute(nn.Module):\n",
    "    def __init__(self, in_filters, out_filters, kernel_size, dropout=0.0, maxpool=0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_filters, out_filters, kernel_size),\n",
    "            nn.BatchNorm1d(out_filters),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        if dropout!=0: self.conv.append(nn.Dropout(dropout))\n",
    "        if maxpool!=0: self.conv.append(nn.MaxPool1d(maxpool))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class DNF(nn.Module):\n",
    "    def __init__(self, in_filters, out_filters, do_transpose=False, do_flatten=False):\n",
    "        super().__init__()\n",
    "        self.do_transpose = do_transpose\n",
    "        self.dnf = nn.Sequential(nn.Linear(in_filters, out_filters))\n",
    "        if do_flatten: self.dnf.append(nn.Flatten())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.do_transpose: x = torch.transpose(x, 2, 1)\n",
    "        return self.dnf(x)\n",
    "\n",
    "\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, in_filters, hidden_filters=50):\n",
    "        super().__init__()\n",
    "        self.ann = nn.Sequential(\n",
    "            nn.Linear(in_filters, hidden_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_filters, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ann(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Convolute(2, 576, 11)\n",
    "        self.conv2 = Convolute(576, 484, 11, 0.3, 4)\n",
    "        self.conv3 = Convolute(484, 400, 5)\n",
    "        self.conv4 = Convolute(400, 324, 5, 0.2)\n",
    "        self.conv5 = DNF(324, 256, True, True)\n",
    "        self.conv6 = DNF(119808, 150)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        return self.conv6(x)\n",
    "\n",
    "class NetCat(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ann1 = ANN(300)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ann1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optim, filename=\"/checkpoint.path.tar\"):\n",
    "    print(\"=> Saving Checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optim_state\": optim.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optim, lr):\n",
    "    print(\"=> Loading Checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    optim.load_state_dict(checkpoint[\"optim_state\"])\n",
    "    for group in optim.param_groups:\n",
    "        group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, net1, net2, netcat, net1_scaler, net2_scaler, netcat_scaler, net1_optim, net2_optim, netcat_optim, loss_function):\n",
    "    losses = []\n",
    "    data_loop = tqdm(data_loader, leave=True)\n",
    "\n",
    "    net1.zero_grad()\n",
    "    net2.zero_grad()\n",
    "    netcat.zero_grad()\n",
    "\n",
    "    for data in data_loop:\n",
    "        features, targets = data['feature'].to(DEVICE), data['target'].to(DEVICE)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            dense1 = net1(features)\n",
    "            dense2 = net2(features)\n",
    "            dense = torch.cat([dense1, dense2], dim = 1)\n",
    "            preds = netcat(dense)\n",
    "            loss = loss_function(preds, targets)\n",
    "\n",
    "        net1.zero_grad()\n",
    "        net1_scaler.scale(loss).backward()\n",
    "        net1_scaler.step(net1_optim)\n",
    "        net1_scaler.update()\n",
    "        net2.zero_grad()\n",
    "        net2_scaler.scale(loss/16).backward()\n",
    "        net2_scaler.step(net2_optim)\n",
    "        net2_scaler.update()\n",
    "        netcat.zero_grad()\n",
    "        netcat_scaler.scale(loss/16).backward()\n",
    "        netcat_scaler.step(netcat_optim)\n",
    "        netcat_scaler.update()\n",
    "\n",
    "        losses.append(loss.data)\n",
    "\n",
    "    loss_avg = torch.mean(torch.FloatTensor(losses))\n",
    "    print(f'Average Loss this epoch = {loss_avg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    net1, net2, netcat = Net(), Net(), NetCat()\n",
    "    net1, net2, netcat = net1.to(DEVICE), net2.to(DEVICE), netcat.to(DEVICE)\n",
    "    net1_scaler, net2_scaler, netcat_scaler = torch.cuda.amp.GradScaler(), torch.cuda.amp.GradScaler(), torch.cuda.amp.GradScaler()\n",
    "    net1_optim = torch.optim.Adam(net1.parameters(), lr=LR, betas=(BETA1, BETA2))\n",
    "    net2_optim = torch.optim.Adam(net2.parameters(), lr=LR, betas=(BETA1, BETA2))\n",
    "    netcat_optim = torch.optim.Adam(netcat.parameters(), lr=LR, betas=(BETA1, BETA2))\n",
    "    loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    train_data = WaveDataset()\n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(NET1_CHK, net1, net1_optim, lr=LR)\n",
    "        load_checkpoint(NET2_CHK, net2, net2_optim, lr=LR)\n",
    "        load_checkpoint(NETCAT_CHK, netcat, netcat_optim, lr=LR)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f'Epoch count = {epoch+1}')\n",
    "        train(train_loader, net1, net2, netcat, net1_scaler, net2_scaler, netcat_scaler, net1_optim, net2_optim, netcat_optim, loss_function)\n",
    "        if SAVE_MODEL and (epoch+1) % 5 == 0:\n",
    "            save_checkpoint(net1, net1_optim, filename=NET1_CHK)\n",
    "            save_checkpoint(net2, net2_optim, filename=NET2_CHK)\n",
    "            save_checkpoint(netcat, netcat_optim, filename=NETCAT_CHK)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1=torch.randn(1, 2, 1925)\n",
    "# head1 = []\n",
    "# head1.append(Convolute(2, 576, 11))\n",
    "# head1.append(Convolute(576, 484, 11, 0.3, 4))\n",
    "# head1.append(Convolute(484, 400, 5))\n",
    "# head1.append(Convolute(400, 324, 5, 0.2))\n",
    "# head1.append(DNF(324, 256, True, True))\n",
    "# head1.append(DNF(119808, 150))\n",
    "# for obj in head1: x1=obj(x1)\n",
    "# x1.shape\n",
    "\n",
    "# x2=torch.randn(1, 2, 1925)\n",
    "# head2 = []\n",
    "# head2.append(Convolute(2, 576, 11))\n",
    "# head2.append(Convolute(576, 484, 11, 0.3, 4))\n",
    "# head2.append(Convolute(484, 400, 5))\n",
    "# head2.append(Convolute(400, 324, 5, 0.2))\n",
    "# head2.append(DNF(324, 256, True, True))\n",
    "# head2.append(DNF(119808, 150))\n",
    "# for obj in head2: x2=obj(x2)\n",
    "# x2.shape\n",
    "\n",
    "# x = torch.cat([x1, x2], dim=1)\n",
    "# x.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f4f1ecbe28321d1e7e48994d3c3c20c7e5b7c7d18dcc157e526614cd30d2d48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
